{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.18'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import featuretools as ft\n",
    "from featuretools.primitives import Day, Weekend, Weekday, Percentile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils_instacart as utils\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from dldb import DLDB, tdfs\n",
    "import os\n",
    "from keras.callbacks import EarlyStopping\n",
    "ft.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = utils.load_entityset('partitioned_data/part_0/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_time = pd.Timestamp('March 1, 2015')\n",
    "training_window = ft.Timedelta(\"60 days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_times = utils.make_labels(es,\n",
    "                                product_name=\"Banana\",\n",
    "                                cutoff_time=cutoff_time,\n",
    "                                prediction_window=ft.Timedelta(\"4 weeks\"),\n",
    "                                training_window=training_window)\n",
    "labels = label_times.set_index('user_id').sort_index()['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create time-stamped feature matrix using DFS\n",
    "\n",
    "We make sure to cutoff the data at the cutoff time, and only use 60 days of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 349it [00:00, 6116.68it/s]\n",
      "Progress: 100%|██████████| 21/21 [08:10<00:00, 23.36s/cutoff time]\n"
     ]
    }
   ],
   "source": [
    "# Note: increasing max_depth from 2 to 3 increases auc from .7 to .9\n",
    "trans_primitives = [Day, Weekend, Weekday, Percentile]\n",
    "fm, fl = tdfs(entityset=es,\n",
    "              target_entity=\"users\",\n",
    "              cutoffs=label_times,\n",
    "              trans_primitives=trans_primitives,\n",
    "              training_window=training_window,\n",
    "              max_depth=3,\n",
    "              window_size='3d',\n",
    "              start=cutoff_time - training_window,\n",
    "              verbose=True)\n",
    "\n",
    "fm = fm.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join all entities in the data together into one dataframe\n",
    "Again, we make sure to cutoff the data at the cutoff time, and only use 60 days of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_denormalized = utils.denormalize_entityset(es, cutoff_time, training_window)\n",
    "fm_denormalized.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DLDB with desired hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model = DLDB(\n",
    "    regression=False,\n",
    "    classes=[False, True],\n",
    "    recurrent_layer_sizes=(32, 32),\n",
    "    dense_layer_sizes=(32, 32),\n",
    "    dropout_fraction=0.2,\n",
    "    recurrent_dropout_fraction=0.1,\n",
    "    categorical_embedding_size=20,\n",
    "    categorical_max_vocab=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the network for DFS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is *slightly* cheating because we give it all the categorical values ahead of time\n",
    "# It most likely won't make a difference, and this step takes some time\n",
    "# Feel free to move it inside of the cross-validation for loop\n",
    "dl_model.compile(fm, fl=fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and test using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 678 samples, validate on 76 samples\n",
      "Epoch 1/100\n",
      "678/678 [==============================] - 4s 6ms/step - loss: 0.5415 - val_loss: 0.4273\n",
      "Epoch 2/100\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4540 - val_loss: 0.3958\n",
      "Epoch 3/100\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4304 - val_loss: 0.3608\n",
      "Epoch 4/100\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3937 - val_loss: 0.3664\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 679 samples, validate on 76 samples\n",
      "Epoch 1/100\n",
      "679/679 [==============================] - 1s 1ms/step - loss: 0.3935 - val_loss: 0.3243\n",
      "Epoch 2/100\n",
      "679/679 [==============================] - 1s 1ms/step - loss: 0.3822 - val_loss: 0.3632\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 679 samples, validate on 76 samples\n",
      "Epoch 1/100\n",
      "679/679 [==============================] - 1s 1ms/step - loss: 0.3568 - val_loss: 0.3263\n",
      "Epoch 2/100\n",
      "679/679 [==============================] - 1s 1ms/step - loss: 0.3720 - val_loss: 0.3274\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "AUC 0.84 +/- 0.10\n"
     ]
    }
   ],
   "source": [
    "cv_score = []\n",
    "n_splits = 3\n",
    "test_frac = 0.1\n",
    "# Use 10% of data as testing set, but only run 3 rounds of cross-validation\n",
    "# (because they take a while)\n",
    "splitter = StratifiedKFold(n_splits=int(1/test_frac), shuffle=True)\n",
    "\n",
    "for i, train_test_index in enumerate(splitter.split(labels, labels)):\n",
    "    train_labels = labels.iloc[train_test_index[0]]\n",
    "    test_labels = labels.iloc[train_test_index[1]]\n",
    "    train_fm = fm.loc[(train_labels.index, slice(None)), :]\n",
    "    test_fm = fm.loc[(test_labels.index, slice(None)), :]\n",
    "\n",
    "\n",
    "    dl_model.fit(\n",
    "        train_fm, train_labels,\n",
    "        # Provide 32 samples to the network at a time\n",
    "        batch_size=32,\n",
    "        # Train on at most 100 passes of the dataset (epochs)\n",
    "        epochs=100,\n",
    "        # After each epoch, test on a held out 10% validation set\n",
    "        validation_split=0.1,\n",
    "        # If no improvement, stop training\n",
    "        callbacks=[EarlyStopping()])\n",
    "    \n",
    "    predictions = dl_model.predict(test_fm)\n",
    "    cv_score.append(roc_auc_score(test_labels, predictions))\n",
    "    if i == n_splits - 1:\n",
    "        break\n",
    "mean_score = np.mean(cv_score)\n",
    "stderr = 2 * (np.std(cv_score) / np.sqrt(n_splits))\n",
    "\n",
    "print(\"AUC %.2f +/- %.2f\" % (mean_score, stderr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the network for the denormalized table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all columns are categorical except the Boolean \"reordered\"\n",
    "categorical_feature_names=[c for c in fm_denormalized.columns if c != 'reordered']\n",
    "dl_model.compile(fm_denormalized,\n",
    "                 categorical_feature_names=categorical_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the denormalized baseline model and test using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 678 samples, validate on 76 samples\n",
      "Epoch 1/100\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.5273 - val_loss: 0.4372\n",
      "Epoch 2/100\n",
      "678/678 [==============================] - 14s 20ms/step - loss: 0.4831 - val_loss: 0.4341\n",
      "Epoch 3/100\n",
      "678/678 [==============================] - 14s 20ms/step - loss: 0.4752 - val_loss: 0.4439\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 679 samples, validate on 76 samples\n",
      "Epoch 1/100\n",
      "679/679 [==============================] - 14s 20ms/step - loss: 0.4737 - val_loss: 0.4526\n",
      "Epoch 2/100\n",
      "679/679 [==============================] - 14s 20ms/step - loss: 0.4783 - val_loss: 0.4492\n",
      "Epoch 3/100\n",
      "679/679 [==============================] - 14s 20ms/step - loss: 0.4465 - val_loss: 0.4660\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 679 samples, validate on 76 samples\n",
      "Epoch 1/100\n",
      "679/679 [==============================] - 14s 20ms/step - loss: 0.4414 - val_loss: 0.4414\n",
      "Epoch 2/100\n",
      "679/679 [==============================] - 14s 20ms/step - loss: 0.4289 - val_loss: 0.4295\n",
      "Epoch 3/100\n",
      "679/679 [==============================] - 14s 21ms/step - loss: 0.4184 - val_loss: 0.3871\n",
      "Epoch 4/100\n",
      "679/679 [==============================] - 14s 21ms/step - loss: 0.3904 - val_loss: 0.3348\n",
      "Epoch 5/100\n",
      "679/679 [==============================] - 14s 21ms/step - loss: 0.3828 - val_loss: 0.3245\n",
      "Epoch 6/100\n",
      "679/679 [==============================] - 14s 21ms/step - loss: 0.3679 - val_loss: 0.3388\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "AUC 0.71 +/- 0.07\n"
     ]
    }
   ],
   "source": [
    "cv_score = []\n",
    "\n",
    "for i, train_test_index in enumerate(splitter.split(labels, labels)):\n",
    "    train_labels = labels.iloc[train_test_index[0]]\n",
    "    test_labels = labels.iloc[train_test_index[1]]\n",
    "    train_fm = fm_denormalized.loc[(train_labels.index, slice(None)), :]\n",
    "    test_fm = fm_denormalized.loc[(test_labels.index, slice(None)), :]\n",
    "\n",
    "\n",
    "    dl_model.fit(\n",
    "        train_fm, train_labels,\n",
    "        # Provide 32 samples to the network at a time\n",
    "        batch_size=32,\n",
    "        # Train on at most 100 passes of the dataset (epochs)\n",
    "        epochs=100,\n",
    "        # After each epoch, test on a held out 10% validation set\n",
    "        validation_split=0.1,\n",
    "        # If no improvement, stop training\n",
    "        callbacks=[EarlyStopping()])\n",
    "    \n",
    "    predictions = dl_model.predict(test_fm)\n",
    "    cv_score.append(roc_auc_score(test_labels, predictions))\n",
    "    if i == n_splits - 1:\n",
    "        break\n",
    "mean_score = np.mean(cv_score)\n",
    "stderr = 2 * (np.std(cv_score) / np.sqrt(n_splits))\n",
    "\n",
    "print(\"AUC %.2f +/- %.2f\" % (mean_score, stderr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
