{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bschreck/miniconda3/envs/py3default/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.1.18'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import featuretools as ft\n",
    "from featuretools.primitives import Day, Weekend, Weekday, Percentile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils_instacart as utils\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from dldb import DLDB, tdfs\n",
    "import os\n",
    "ft.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLDB: Using DFS for Smaller, Easier to Train Recurrent Neural Networks\n",
    "\n",
    "Deep Feature Synthesis works on time-varying, transactional-level data to generate powerful, interpretable features for machine learning. Vanilla DFS produces a 2-dimensional feature matrix that can be used for classic machine learning techniques, such as SVM or Random Forest. These techniques need a fixed-size feature matrix where any time-dependence is summarized into historical statistics (e.g. Number of items a customer purchased in the past 30 days).\n",
    "\n",
    "We can take more explicit advantage of the time-dimension in this type of data using Recurrent Neural Networks. RNNs take in sequences of features, where the 3rd dimension in our case would represent time. Since RNNs learn high-level features on their own, the usual approach when using multiple tables is just to join all of them together and use the raw values. We will show that approach as a baseline here.\n",
    "\n",
    "Instead, we can use DFS to produce high-level features at different points in time, and then learn from these sequences of features, rather than raw data. This strategy essentially encodes prior human intuition and assumptions about relevant data transformations into the problem before letting the deep learning do its thing. Because the net doesn't have to learn every feature from scratch, we may be able to reduce training time, use a simpler net, not have to tweak as many hyperparameters, use less data, or boost performance. In this notebook, we will show that with a relatively simple network we can achieve pretty good AUC with the DFS features before the raw data network learns much of anything.\n",
    "\n",
    "We'll try it on Instacart data, solving the same problem as this [previous Featuretools demo](https://github.com/Featuretools/predict_next_purchase), which used a Random Forest to predict whether a customer will buy a banana in their next Instacart order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLDB Library\n",
    "\n",
    "[DLDB](https://github.com/HDI-Project/DL-DB) is a utility library for building recurrent neural networks from a feature matrix with multiple cutoff times per instance. Internally, it uses the [Keras](keras.io) library (which in turn uses [Tensor Flow](tensorflow.org)). \n",
    "\n",
    "It works by mapping each categorical feature to a Keras Embedding layer in order to transform it into a dense, numeric vector. Then all the inputs are fed into several recurrent layers (specified in hyperparameters) and several feed-forward layers (also specified in hyperparameters). It also includes an optional 1-D convolutional layer that will be applied before the recurrent layers. \n",
    "\n",
    "We packaged DL-DB into a Python library that can be installed via pip:\n",
    "    \n",
    "```\n",
    "pip install dldb\n",
    "```\n",
    "\n",
    "This library includes both a class to build these recurrent neural network models as well as a wrapper function around Featuretools (`tdfs()`) to create time-series features as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in the data\n",
    "\n",
    "The data is partitioned into chunks based on `user_id`, and loaded into the Featuretools Entityset format. See [the original demo](https://github.com/Featuretools/predict_next_purchase) for more explananation about how the data is partitioned and the Entityset is formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = utils.load_entityset('partitioned_data/part_0/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct labels\n",
    "\n",
    "This utility function picks out a window of time, and finds which users bought bananas. Again, more explanation in [the original demo](https://github.com/Featuretools/predict_next_purchase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_time = pd.Timestamp('March 1, 2015')\n",
    "training_window = ft.Timedelta(\"60 days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_times = utils.make_labels(es,\n",
    "                                product_name=\"Banana\",\n",
    "                                cutoff_time=cutoff_time,\n",
    "                                prediction_window=ft.Timedelta(\"4 weeks\"),\n",
    "                                training_window=training_window)\n",
    "labels = label_times.set_index('user_id').sort_index()['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create time-stamped feature matrix using DFS\n",
    "\n",
    "Here is where things start to get interesting. We use the [`tdfs` function in DLDB](https://github.com/HDI-Project/DL-DB/blob/master/dldb/tdfs.py) to produce a feature matrix with several rows per user. It works by adding additional cutoff times in the past to each `(user_id, cutoff_time)` provided in `label_times`.\n",
    "\n",
    "This function has a few different ways of selecting these additional cutoff times. Here, we provide `window_size='3d'` and `start=cutoff_time - training_window`, which will go back in time in increments of 3 days until 60 days before the cutoff time of March 1st. This produces a sequence of 20 cutoff times per user.\n",
    "\n",
    "We could have also specified `num_windows=20` and `window_size=3d` to produce the same result.\n",
    "\n",
    "The rest of the arguments are standard DFS arguments. For an overview of DFS, check out the [Featuretools documentation](https://docs.featuretools.com/automated_feature_engineering/afe.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 121it [00:00, 6344.76it/s]\n",
      "Progress: 100%|██████████| 21/21 [02:55<00:00,  8.37s/cutoff time]\n"
     ]
    }
   ],
   "source": [
    "trans_primitives = [Day, Weekend, Weekday, Percentile]\n",
    "fm, fl = tdfs(entityset=es,\n",
    "              target_entity=\"users\",\n",
    "              cutoffs=label_times,\n",
    "              trans_primitives=trans_primitives,\n",
    "              training_window=training_window,\n",
    "              max_depth=2,\n",
    "              window_size='3d',\n",
    "              start=cutoff_time - training_window,\n",
    "              verbose=True)\n",
    "\n",
    "fm = fm.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can save/restore our work without having to recompute feature matrix\n",
    "#fm.to_csv('fm_part_0.csv')\n",
    "#fm = pd.read_csv('fm_part_0.csv', parse_dates=['time'], index_col=['user_id', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ft.save_features(fl, 'fl_part_0.p')\n",
    "#fl = ft.load_features('fl_part_0.p', es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Baseline Input Data\n",
    "\n",
    "This \"feature_matrix\" is created by joining all entities in the data together into one dataframe.\n",
    "Just like the feature matrix created from `tdfs`, we make sure to cutoff the data at the cutoff time, and only use 60 days of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_denormalized = utils.denormalize_entityset(es, cutoff_time, training_window)\n",
    "fm_denormalized.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DLDB with desired hyperparameters\n",
    "\n",
    "In this example, we use 2 fairly small [LSTM](https://keras.io/layers/recurrent/) layers and 2 feed-forward layers (called \"Dense layers\" in Keras/Tensor Flow terminology). DLDB has an extremely simple API, and exposes a large number of hyperparameters, so is amenable to hyperparameter optimization algorithms.\n",
    "\n",
    "Each categorical feature will be mapped to a 12-dimensional embedding, with a maximum of 20 unique categorical values (the top 20 most frequent values will be chosen, and the rest will be converted to a single token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model = DLDB(\n",
    "    regression=False,\n",
    "    classes=[False, True],\n",
    "    recurrent_layer_sizes=(32, 32),\n",
    "    dense_layer_sizes=(32, 16),\n",
    "    dropout_fraction=0.2,\n",
    "    recurrent_dropout_fraction=0.2,\n",
    "    categorical_embedding_size=12,\n",
    "    categorical_max_vocab=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the network\n",
    "\n",
    "**Note** Doing this step outside of the cross-validation loop is *slightly* cheating because we give it all the categorical values ahead of time. It most likely won't make a difference, and this step takes some time.\n",
    "\n",
    "Feel free to move it inside of the cross-validation for loop and see how much the results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model.compile(fm, fl=fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and test using cross-validation\n",
    "\n",
    "We use a `batch_size` of 128 (for each gradient update step) and train over 3 passes of the dataset (epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 678 samples, validate on 76 samples\n",
      "Epoch 1/3\n",
      "678/678 [==============================] - 4s 6ms/step - loss: 0.6496 - val_loss: 0.4914\n",
      "Epoch 2/3\n",
      "678/678 [==============================] - 0s 457us/step - loss: 0.4948 - val_loss: 0.4523\n",
      "Epoch 3/3\n",
      "678/678 [==============================] - 0s 450us/step - loss: 0.4661 - val_loss: 0.4296\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 679 samples, validate on 76 samples\n",
      "Epoch 1/3\n",
      "679/679 [==============================] - 4s 6ms/step - loss: 0.5909 - val_loss: 0.4419\n",
      "Epoch 2/3\n",
      "679/679 [==============================] - 0s 461us/step - loss: 0.4626 - val_loss: 0.4186\n",
      "Epoch 3/3\n",
      "679/679 [==============================] - 0s 453us/step - loss: 0.4614 - val_loss: 0.4187\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 679 samples, validate on 76 samples\n",
      "Epoch 1/3\n",
      "679/679 [==============================] - 4s 7ms/step - loss: 0.6741 - val_loss: 0.5897\n",
      "Epoch 2/3\n",
      "679/679 [==============================] - 0s 508us/step - loss: 0.5437 - val_loss: 0.4291\n",
      "Epoch 3/3\n",
      "679/679 [==============================] - 0s 449us/step - loss: 0.4934 - val_loss: 0.4180\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 679 samples, validate on 76 samples\n",
      "Epoch 1/3\n",
      "679/679 [==============================] - 4s 6ms/step - loss: 0.6601 - val_loss: 0.5431\n",
      "Epoch 2/3\n",
      "679/679 [==============================] - 0s 439us/step - loss: 0.5038 - val_loss: 0.4875\n",
      "Epoch 3/3\n",
      "679/679 [==============================] - 0s 444us/step - loss: 0.4840 - val_loss: 0.4441\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 679 samples, validate on 76 samples\n",
      "Epoch 1/3\n",
      "679/679 [==============================] - 5s 7ms/step - loss: 0.6450 - val_loss: 0.5042\n",
      "Epoch 2/3\n",
      "679/679 [==============================] - 0s 491us/step - loss: 0.4687 - val_loss: 0.4474\n",
      "Epoch 3/3\n",
      "679/679 [==============================] - 0s 477us/step - loss: 0.4737 - val_loss: 0.4390\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 679 samples, validate on 76 samples\n",
      "Epoch 1/3\n",
      "679/679 [==============================] - 5s 7ms/step - loss: 0.6515 - val_loss: 0.5230\n",
      "Epoch 2/3\n",
      "679/679 [==============================] - 0s 476us/step - loss: 0.4997 - val_loss: 0.4557\n",
      "Epoch 3/3\n",
      "679/679 [==============================] - 0s 453us/step - loss: 0.4739 - val_loss: 0.4441\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 679 samples, validate on 76 samples\n",
      "Epoch 1/3\n",
      "679/679 [==============================] - 6s 9ms/step - loss: 0.6558 - val_loss: 0.4983\n",
      "Epoch 2/3\n",
      "679/679 [==============================] - 0s 455us/step - loss: 0.4966 - val_loss: 0.4233\n",
      "Epoch 3/3\n",
      "679/679 [==============================] - 0s 470us/step - loss: 0.4560 - val_loss: 0.3970\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 679 samples, validate on 76 samples\n",
      "Epoch 1/3\n",
      "679/679 [==============================] - 5s 8ms/step - loss: 0.6400 - val_loss: 0.4516\n",
      "Epoch 2/3\n",
      "679/679 [==============================] - 0s 463us/step - loss: 0.4798 - val_loss: 0.4115\n",
      "Epoch 3/3\n",
      "679/679 [==============================] - 0s 453us/step - loss: 0.4670 - val_loss: 0.3809\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 680 samples, validate on 76 samples\n",
      "Epoch 1/3\n",
      "680/680 [==============================] - 6s 9ms/step - loss: 0.6783 - val_loss: 0.6196\n",
      "Epoch 2/3\n",
      "680/680 [==============================] - 0s 444us/step - loss: 0.5432 - val_loss: 0.4747\n",
      "Epoch 3/3\n",
      "680/680 [==============================] - 0s 446us/step - loss: 0.4821 - val_loss: 0.4513\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 680 samples, validate on 76 samples\n",
      "Epoch 1/3\n",
      "680/680 [==============================] - 6s 9ms/step - loss: 0.6532 - val_loss: 0.5037\n",
      "Epoch 2/3\n",
      "680/680 [==============================] - 0s 431us/step - loss: 0.4866 - val_loss: 0.4203\n",
      "Epoch 3/3\n",
      "680/680 [==============================] - 0s 437us/step - loss: 0.4578 - val_loss: 0.3959\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "AUC 0.72 +/- 0.05\n"
     ]
    }
   ],
   "source": [
    "cv_score = []\n",
    "n_splits=10\n",
    "splitter = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "for i, train_test_index in enumerate(splitter.split(labels, labels)):\n",
    "    train_labels = labels.iloc[train_test_index[0]]\n",
    "    test_labels = labels.iloc[train_test_index[1]]\n",
    "    train_fm = fm.loc[(train_labels.index, slice(None)), :]\n",
    "    test_fm = fm.loc[(test_labels.index, slice(None)), :]\n",
    "\n",
    "\n",
    "    dl_model.fit(\n",
    "        train_fm, train_labels,\n",
    "        # Provide this many samples to the network at a time\n",
    "        batch_size=128,\n",
    "        epochs=3,\n",
    "        # After each epoch, test on a held out 10% validation set\n",
    "        validation_split=0.1)\n",
    "    \n",
    "    predictions = dl_model.predict(test_fm)\n",
    "    cv_score.append(roc_auc_score(test_labels, predictions))\n",
    "mean_score = np.mean(cv_score)\n",
    "stderr = 2 * (np.std(cv_score) / np.sqrt(n_splits))\n",
    "\n",
    "print(\"AUC %.2f +/- %.2f\" % (mean_score, stderr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the baseline model over raw data and test using cross-validation\n",
    "\n",
    "We use the same parameters here. Note that we tell DL-DB explicitly what feature names are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all columns are categorical except the Boolean \"reordered\"\n",
    "categorical_feature_names=[c for c in fm_denormalized.columns if c != 'reordered']\n",
    "\n",
    "\n",
    "dl_model.compile(fm_denormalized, categorical_feature_names=categorical_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 678 samples, validate on 76 samples\n",
      "Epoch 1/3\n",
      "678/678 [==============================] - 11s 16ms/step - loss: 0.6282 - val_loss: 0.5159\n",
      "Epoch 2/3\n",
      "678/678 [==============================] - 4s 6ms/step - loss: 0.5040 - val_loss: 0.4985\n",
      "Epoch 3/3\n",
      "678/678 [==============================] - 4s 6ms/step - loss: 0.4984 - val_loss: 0.5073\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 679 samples, validate on 76 samples\n",
      "Epoch 1/3\n",
      "679/679 [==============================] - 10s 15ms/step - loss: 0.6477 - val_loss: 0.5111\n",
      "Epoch 2/3\n",
      "679/679 [==============================] - 4s 6ms/step - loss: 0.4895 - val_loss: 0.5238\n",
      "Epoch 3/3\n",
      "679/679 [==============================] - 4s 7ms/step - loss: 0.4828 - val_loss: 0.4874\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 679 samples, validate on 76 samples\n",
      "Epoch 1/3\n",
      "679/679 [==============================] - 11s 17ms/step - loss: 0.6129 - val_loss: 0.4705\n",
      "Epoch 2/3\n",
      "679/679 [==============================] - 4s 6ms/step - loss: 0.5012 - val_loss: 0.4627\n",
      "Epoch 3/3\n",
      "679/679 [==============================] - 4s 6ms/step - loss: 0.4905 - val_loss: 0.4597\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "Transforming input matrix into numeric sequences\n",
      "Fitting Keras model\n",
      "Train on 679 samples, validate on 76 samples\n",
      "Epoch 1/3\n",
      "679/679 [==============================] - 10s 15ms/step - loss: 0.6299 - val_loss: 0.5208\n",
      "Epoch 2/3\n",
      "679/679 [==============================] - 7s 10ms/step - loss: 0.4883 - val_loss: 0.5074\n",
      "Epoch 3/3\n",
      "679/679 [==============================] - 4s 6ms/step - loss: 0.4849 - val_loss: 0.4973\n",
      "Transforming input matrix into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "AUC 0.54 +/- 0.06\n"
     ]
    }
   ],
   "source": [
    "cv_score = []\n",
    "\n",
    "for i, train_test_index in enumerate(splitter.split(labels, labels)):\n",
    "    train_labels = labels.iloc[train_test_index[0]]\n",
    "    test_labels = labels.iloc[train_test_index[1]]\n",
    "    train_fm = fm_denormalized.loc[(train_labels.index, slice(None)), :]\n",
    "    test_fm = fm_denormalized.loc[(test_labels.index, slice(None)), :]\n",
    "\n",
    "\n",
    "    dl_model.fit(\n",
    "        train_fm, train_labels,\n",
    "        # Provide this many samples to the network at a time\n",
    "        batch_size=128,\n",
    "        epochs=3,\n",
    "        # After each epoch, test on a held out 10% validation set\n",
    "        validation_split=0.1)\n",
    "    \n",
    "    predictions = dl_model.predict(test_fm)\n",
    "    cv_score.append(roc_auc_score(test_labels, predictions))\n",
    "    \n",
    "    if i == 3:\n",
    "        break\n",
    "mean_score = np.mean(cv_score)\n",
    "stderr = 2 * (np.std(cv_score) / np.sqrt(n_splits))\n",
    "\n",
    "print(\"AUC %.2f +/- %.2f\" % (mean_score, stderr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "The model using DFS features scored over 30% better AUC than the raw data model for the same parameters.\n",
    "\n",
    "Try increasing the number of epochs in the raw data model- eventually you will end up with similar scores as the DFS model. I found that increasing from 3 to 7 epochs increases the raw data score from .53 to .66.\n",
    "\n",
    "This is an interesting result, and hints at the idea that using good features to start out with can reduce the training time of deep learning models.\n",
    "\n",
    "There are many more ideas we can test here:\n",
    " * What happens as we increase/decrease the amount of data we use? Remember that we used only a single partition (out of over 100)\n",
    " * How many rounds of hyperparameter optimization do we have to do to achieve the same result?\n",
    " * What if we use a more complex network?\n",
    " * Is it possible to visualize the effect of the input features on the LSTM network? This is a hard problem in deep learning in general"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
