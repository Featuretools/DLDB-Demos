{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bschreck/miniconda3/envs/py3default/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.1.20'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import featuretools as ft\n",
    "from featuretools.primitives import NumTrue, PercentTrue\n",
    "from featuretools.selection import remove_low_information_features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils_backblaze as utils\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import (RandomForestClassifier,\n",
    "                              RandomForestRegressor)\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from dldb import DLDB\n",
    "import os\n",
    "ft.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLDB: Using DFS to Train Recurrent Neural Networks\n",
    "\n",
    "\n",
    "### Brief DFS primer\n",
    "Deep Feature Synthesis (DFS) works on time-varying, transactional-level data to generate powerful, interpretable features for machine learning. Raw data consists of many tables, with some columns acting as links between tables. We want to produce a fixed-size feature vector for each row of one of these tables, but taking advantage of the data contained in the other tables. DFS generates these feature vectors by applying many statistical functions, called primitives, across tables. And importantly, it generates these features at specific moments in time, taking precautions to only use data from before the desired time.\n",
    "\n",
    "For instance, the data we will use in this notebook contains a table with a row for each Instacart user, and several other tables about their shopping behavior. DFS can apply the \"sum\" primitive to the dollar amount of each order per user, producing a feature for \"the total amount spent on Instacart per user\". Adding a *cutoff time* of March 1, 2015, the feature becomes the \"total amount spent on Instacart per user before March 1, 2015\". DFS can also combine several primitives, allowing it to form features like the \"standard deviation of the number of items in each user's previous orders\".\n",
    "\n",
    "For a more in depth explanation of DFS, we encourage you to check out this [blog post](https://www.featurelabs.com/blog/deep-feature-synthesis/) and [this page](https://docs.featuretools.com/automated_feature_engineering/afe.html) in the Featuretools documentation.\n",
    "\n",
    "### Producing a 3D tensor from DFS\n",
    "DFS as described produces a 2-dimensional feature matrix that can be used for classic machine learning techniques, such as SVM or Random Forest. These techniques need a fixed-size feature matrix where any time-dependence is summarized into historical statistics (e.g. Number of items a customer purchased in the past 30 days).\n",
    "\n",
    "We can take more explicit advantage of the time-dimension in this type of data using Recurrent Neural Networks. RNNs take in sequences of features, where the 3rd dimension in our case would represent time. Since RNNs learn high-level features on their own, the usual approach when using multiple tables is just to join all of them together and use the raw values. We will show that approach as a baseline here.\n",
    "\n",
    "Instead, we can use DFS to produce high-level features at different points in time, and then learn from these sequences of features, rather than raw data. In this case, we would use DFS to produce a 3D tensor flattened as a 2D matrix, with multiple times for each instance. Combining DFS with RNNs essentially encodes prior human intuition and assumptions about relevant data transformations into the problem before letting the deep learning do its thing. Because the net doesn't have to learn every feature from scratch, we may be able to reduce training time, use a simpler net, not have to tweak as many hyperparameters, use less data, or boost performance. In this notebook, we will show a network using DFS features that produces higher scores with less variation than the same network trained on raw data.\n",
    "\n",
    "We'll try it on [harddrive failure data from Backblaze](https://www.backblaze.com/b2/hard-drive-test-data.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLDB Library\n",
    "\n",
    "[DLDB](https://github.com/HDI-Project/DL-DB) is a utility library for building recurrent neural networks from a feature matrix with multiple cutoff times per instance. Internally, it uses the [Keras](keras.io) library (which in turn uses [Tensor Flow](tensorflow.org)). \n",
    "\n",
    "It works by first imputing and scaling a sequence feature matrix (the result of calling `tdfs()`), and then separating the numeric features from the categoricals. Each categorical feature is mapped to a Keras Embedding layer in order to transform it into a dense, numeric vector. Then these embeddings are concatenated with the numeric features and fed into several recurrent layers (specified in hyperparameters) and several feed-forward layers (also specified in hyperparameters). It also includes an optional 1-D convolutional layer that will be applied before the recurrent layers. All the network layers, including the categorical embeddings, are trained end-to-end using any gradient update methods available in Keras.\n",
    "\n",
    "We packaged DL-DB into a Python library that can be installed via pip:\n",
    "    \n",
    "```\n",
    "pip install dldb\n",
    "```\n",
    "\n",
    "This library includes both a class to build these recurrent neural network models as well as the `tdfs()` function that creates time-series features as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in the data\n",
    "\n",
    "The data is loaded from many individual CSV files, and then converted into the Featuretools Entityset format.\n",
    "\n",
    "To make this notebook more interactive and because the data is heavily imbalanced toward working hard drives, we downsample the \"negative class\". A positive label means that a hard drive failed on the subsequent day, while a negative means that it did not. To do this downsampling, we remove 90% of the hard drives that never failed across the duration of the available CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/bschreck/Google Drive File Stream/My Drive/Feature Labs Shared/EntitySets/entitysets/backblaze_harddrive/data\"\n",
    "df = utils.load_data_as_dataframe(data_dir=data_dir, csv_glob='*.csv',\n",
    "                                  negative_downsample_frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bschreck/miniconda3/envs/py3default/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: 'serial_number' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    850\n",
       "True     321\n",
       "Name: failure, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('serial_number')['failure'].last().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: BackBlaze\n",
       "  Entities:\n",
       "    SMART_observations [Rows: 67839, Columns: 94]\n",
       "    HDD [Rows: 1171, Columns: 4]\n",
       "    models [Rows: 22, Columns: 1]\n",
       "  Relationships:\n",
       "    SMART_observations.serial_number -> HDD.serial_number\n",
       "    HDD.model -> models.model"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = utils.load_entityset_from_dataframe(df)\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct labels\n",
    "\n",
    "This utility function picks out a sampling of hard drives at particular days in their lifecycle, and labels each as True or False depending on whether they failed the following day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_window = \"20 days\"\n",
    "lead = pd.Timedelta('1 day')\n",
    "prediction_window = pd.Timedelta('25 days')\n",
    "min_training_data = pd.Timedelta('5 days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating labels...: 100%|██████████| 1172/1172 [00:02<00:00, 415.31it/s]\n"
     ]
    }
   ],
   "source": [
    "labels = utils.create_labels(es,\n",
    "                             lead,\n",
    "                             min_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    849\n",
       "True     282\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create time-stamped feature tensor using DFS\n",
    "\n",
    "Here is where things start to get interesting. We use the [`make_temporal_cutoffs` function in Featuretools](https://github.com/HDI-Project/DL-DB/blob/master/dldb/tdfs.py) to produce a serious of preceding cutoff times for each label/cutoff time pair. We then provide these cutoffs to DFS to generate a feature tensor with several rows per harddrive serial number.\n",
    "\n",
    "This `make_temporal_cutoffs` function has a few different ways of selecting these additional cutoff times. Here, we provide `window_size='1d'` and `starts` equal to the first recorded time for each drive. This produces sequences spaced out by 1 day (the frequency of recording in the actual dataset) from the first recording until the cutoff time, at which point we have to make a prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_ids = labels.index.get_level_values('serial_number')\n",
    "cutoffs = labels.index.get_level_values('cutoff')\n",
    "starts = es['SMART_observations'].df.groupby('serial_number')['date'].min().loc[instance_ids].tolist()\n",
    "temporal_cutoffs = ft.make_temporal_cutoffs(instance_ids=instance_ids,\n",
    "                                            cutoffs=cutoffs,\n",
    "                                            start=starts,\n",
    "                                            window_size='1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 1117 features\n",
      "Elapsed: 44:43 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 11/11 chunks\n"
     ]
    }
   ],
   "source": [
    "trans_primitives = [\"day\", \"days\"]\n",
    "label_feature = ft.Feature(es[\"SMART_observations\"][\"failure\"]) == 1\n",
    "seed_features = [label_feature,\n",
    "                 NumTrue(label_feature, es[\"HDD\"]), \n",
    "                 PercentTrue(label_feature, es[\"HDD\"])]\n",
    "ftens, fl = ft.dfs(entityset=es,\n",
    "                target_entity=\"HDD\",\n",
    "                cutoff_time=temporal_cutoffs,\n",
    "                cutoff_time_in_index=True,\n",
    "                trans_primitives=[\"day\", \"days\"],\n",
    "                seed_features=seed_features,\n",
    "                max_depth=2,\n",
    "                verbose=True)\n",
    "# Make sure ftens is sorted the same way as the labels\n",
    "ftens = ftens.swaplevel(i=1, j=0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>capacity_bytes</th>\n",
       "      <th>SUM(SMART_observations.smart_10_normalized)</th>\n",
       "      <th>SUM(SMART_observations.smart_10_raw)</th>\n",
       "      <th>SUM(SMART_observations.smart_11_normalized)</th>\n",
       "      <th>SUM(SMART_observations.smart_11_raw)</th>\n",
       "      <th>SUM(SMART_observations.smart_12_normalized)</th>\n",
       "      <th>SUM(SMART_observations.smart_12_raw)</th>\n",
       "      <th>SUM(SMART_observations.smart_13_normalized)</th>\n",
       "      <th>SUM(SMART_observations.smart_13_raw)</th>\n",
       "      <th>...</th>\n",
       "      <th>models.STD(HDD.NUM_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.STD(HDD.PERCENT_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.MAX(HDD.NUM_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.MAX(HDD.PERCENT_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.SKEW(HDD.NUM_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.SKEW(HDD.PERCENT_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.MIN(HDD.NUM_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.MIN(HDD.PERCENT_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.MEAN(HDD.NUM_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.MEAN(HDD.PERCENT_TRUE(SMART_observations.failure = 1))</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th>serial_number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2017-01-01</th>\n",
       "      <th>45CHK11WFMYB</th>\n",
       "      <td>TOSHIBA MD04ABA400V</td>\n",
       "      <td>4.000787e+12</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45D7K132FMYB</th>\n",
       "      <td>TOSHIBA MD04ABA400V</td>\n",
       "      <td>4.000787e+12</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641SFR3S</th>\n",
       "      <td>TOSHIBA MQ01ABF050</td>\n",
       "      <td>5.001079e+11</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641SFRIS</th>\n",
       "      <td>TOSHIBA MQ01ABF050</td>\n",
       "      <td>5.001079e+11</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MJ0351YNG9P2NA</th>\n",
       "      <td>Hitachi HDS5C3030ALA630</td>\n",
       "      <td>3.000593e+12</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             model  capacity_bytes  \\\n",
       "time       serial_number                                             \n",
       "2017-01-01 45CHK11WFMYB        TOSHIBA MD04ABA400V    4.000787e+12   \n",
       "           45D7K132FMYB        TOSHIBA MD04ABA400V    4.000787e+12   \n",
       "           5641SFR3S            TOSHIBA MQ01ABF050    5.001079e+11   \n",
       "           5641SFRIS            TOSHIBA MQ01ABF050    5.001079e+11   \n",
       "           MJ0351YNG9P2NA  Hitachi HDS5C3030ALA630    3.000593e+12   \n",
       "\n",
       "                           SUM(SMART_observations.smart_10_normalized)  \\\n",
       "time       serial_number                                                 \n",
       "2017-01-01 45CHK11WFMYB                                          100.0   \n",
       "           45D7K132FMYB                                          100.0   \n",
       "           5641SFR3S                                             100.0   \n",
       "           5641SFRIS                                             100.0   \n",
       "           MJ0351YNG9P2NA                                        100.0   \n",
       "\n",
       "                           SUM(SMART_observations.smart_10_raw)  \\\n",
       "time       serial_number                                          \n",
       "2017-01-01 45CHK11WFMYB                                     0.0   \n",
       "           45D7K132FMYB                                     0.0   \n",
       "           5641SFR3S                                        0.0   \n",
       "           5641SFRIS                                        0.0   \n",
       "           MJ0351YNG9P2NA                                   0.0   \n",
       "\n",
       "                           SUM(SMART_observations.smart_11_normalized)  \\\n",
       "time       serial_number                                                 \n",
       "2017-01-01 45CHK11WFMYB                                            0.0   \n",
       "           45D7K132FMYB                                            0.0   \n",
       "           5641SFR3S                                               0.0   \n",
       "           5641SFRIS                                               0.0   \n",
       "           MJ0351YNG9P2NA                                          0.0   \n",
       "\n",
       "                           SUM(SMART_observations.smart_11_raw)  \\\n",
       "time       serial_number                                          \n",
       "2017-01-01 45CHK11WFMYB                                     0.0   \n",
       "           45D7K132FMYB                                     0.0   \n",
       "           5641SFR3S                                        0.0   \n",
       "           5641SFRIS                                        0.0   \n",
       "           MJ0351YNG9P2NA                                   0.0   \n",
       "\n",
       "                           SUM(SMART_observations.smart_12_normalized)  \\\n",
       "time       serial_number                                                 \n",
       "2017-01-01 45CHK11WFMYB                                          100.0   \n",
       "           45D7K132FMYB                                          100.0   \n",
       "           5641SFR3S                                             100.0   \n",
       "           5641SFRIS                                             100.0   \n",
       "           MJ0351YNG9P2NA                                        100.0   \n",
       "\n",
       "                           SUM(SMART_observations.smart_12_raw)  \\\n",
       "time       serial_number                                          \n",
       "2017-01-01 45CHK11WFMYB                                     2.0   \n",
       "           45D7K132FMYB                                     3.0   \n",
       "           5641SFR3S                                        4.0   \n",
       "           5641SFRIS                                        5.0   \n",
       "           MJ0351YNG9P2NA                                  14.0   \n",
       "\n",
       "                           SUM(SMART_observations.smart_13_normalized)  \\\n",
       "time       serial_number                                                 \n",
       "2017-01-01 45CHK11WFMYB                                            0.0   \n",
       "           45D7K132FMYB                                            0.0   \n",
       "           5641SFR3S                                               0.0   \n",
       "           5641SFRIS                                               0.0   \n",
       "           MJ0351YNG9P2NA                                          0.0   \n",
       "\n",
       "                           SUM(SMART_observations.smart_13_raw)  \\\n",
       "time       serial_number                                          \n",
       "2017-01-01 45CHK11WFMYB                                     0.0   \n",
       "           45D7K132FMYB                                     0.0   \n",
       "           5641SFR3S                                        0.0   \n",
       "           5641SFRIS                                        0.0   \n",
       "           MJ0351YNG9P2NA                                   0.0   \n",
       "\n",
       "                                                       ...                                \\\n",
       "time       serial_number                               ...                                 \n",
       "2017-01-01 45CHK11WFMYB                                ...                                 \n",
       "           45D7K132FMYB                                ...                                 \n",
       "           5641SFR3S                                   ...                                 \n",
       "           5641SFRIS                                   ...                                 \n",
       "           MJ0351YNG9P2NA                              ...                                 \n",
       "\n",
       "                           models.STD(HDD.NUM_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                              \n",
       "2017-01-01 45CHK11WFMYB                                                  0.0          \n",
       "           45D7K132FMYB                                                  0.0          \n",
       "           5641SFR3S                                                     0.0          \n",
       "           5641SFRIS                                                     0.0          \n",
       "           MJ0351YNG9P2NA                                                0.0          \n",
       "\n",
       "                           models.STD(HDD.PERCENT_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                                  \n",
       "2017-01-01 45CHK11WFMYB                                                  0.0              \n",
       "           45D7K132FMYB                                                  0.0              \n",
       "           5641SFR3S                                                     0.0              \n",
       "           5641SFRIS                                                     0.0              \n",
       "           MJ0351YNG9P2NA                                                0.0              \n",
       "\n",
       "                           models.MAX(HDD.NUM_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                              \n",
       "2017-01-01 45CHK11WFMYB                                                  0.0          \n",
       "           45D7K132FMYB                                                  0.0          \n",
       "           5641SFR3S                                                     0.0          \n",
       "           5641SFRIS                                                     0.0          \n",
       "           MJ0351YNG9P2NA                                                0.0          \n",
       "\n",
       "                           models.MAX(HDD.PERCENT_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                                  \n",
       "2017-01-01 45CHK11WFMYB                                                  0.0              \n",
       "           45D7K132FMYB                                                  0.0              \n",
       "           5641SFR3S                                                     0.0              \n",
       "           5641SFRIS                                                     0.0              \n",
       "           MJ0351YNG9P2NA                                                0.0              \n",
       "\n",
       "                           models.SKEW(HDD.NUM_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                               \n",
       "2017-01-01 45CHK11WFMYB                                                  0.0           \n",
       "           45D7K132FMYB                                                  0.0           \n",
       "           5641SFR3S                                                     0.0           \n",
       "           5641SFRIS                                                     0.0           \n",
       "           MJ0351YNG9P2NA                                                0.0           \n",
       "\n",
       "                           models.SKEW(HDD.PERCENT_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                                   \n",
       "2017-01-01 45CHK11WFMYB                                                  0.0               \n",
       "           45D7K132FMYB                                                  0.0               \n",
       "           5641SFR3S                                                     0.0               \n",
       "           5641SFRIS                                                     0.0               \n",
       "           MJ0351YNG9P2NA                                                0.0               \n",
       "\n",
       "                           models.MIN(HDD.NUM_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                              \n",
       "2017-01-01 45CHK11WFMYB                                                  0.0          \n",
       "           45D7K132FMYB                                                  0.0          \n",
       "           5641SFR3S                                                     0.0          \n",
       "           5641SFRIS                                                     0.0          \n",
       "           MJ0351YNG9P2NA                                                0.0          \n",
       "\n",
       "                           models.MIN(HDD.PERCENT_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                                  \n",
       "2017-01-01 45CHK11WFMYB                                                  0.0              \n",
       "           45D7K132FMYB                                                  0.0              \n",
       "           5641SFR3S                                                     0.0              \n",
       "           5641SFRIS                                                     0.0              \n",
       "           MJ0351YNG9P2NA                                                0.0              \n",
       "\n",
       "                           models.MEAN(HDD.NUM_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                               \n",
       "2017-01-01 45CHK11WFMYB                                                  0.0           \n",
       "           45D7K132FMYB                                                  0.0           \n",
       "           5641SFR3S                                                     0.0           \n",
       "           5641SFRIS                                                     0.0           \n",
       "           MJ0351YNG9P2NA                                                0.0           \n",
       "\n",
       "                           models.MEAN(HDD.PERCENT_TRUE(SMART_observations.failure = 1))  \n",
       "time       serial_number                                                                  \n",
       "2017-01-01 45CHK11WFMYB                                                  0.0              \n",
       "           45D7K132FMYB                                                  0.0              \n",
       "           5641SFR3S                                                     0.0              \n",
       "           5641SFRIS                                                     0.0              \n",
       "           MJ0351YNG9P2NA                                                0.0              \n",
       "\n",
       "[5 rows x 1117 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting features\n",
    "DFS generates over 1000 features for this dataset. Many of them won't be useful, so we an do a pass of supervised feature selection before building the deep learning model.\n",
    "\n",
    "NOTE: if DFS produces fewer features (100 is a good maximum), then no feature selection is necessary\n",
    "\n",
    "To do this, we use a Random Forest Classifier's built-in feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, one-hot-encode categoricals and drop zero-variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftens, fl = ft.encode_features(ftens, fl)\n",
    "ftens, fl = remove_low_information_features(ftens, fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, impute missing values and train a Random Forest on the last cutoff time for each hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = RandomForestClassifier(n_estimators=1000, class_weight='balanced', n_jobs=-1, verbose=True)\n",
    "imputer = Imputer(missing_values='NaN', strategy=\"mean\", axis=0)\n",
    "selector = SelectFromModel(est, threshold=\"mean\")\n",
    "pipeline = Pipeline([(\"imputer\", imputer),(\"selector\", selector)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = ftens.groupby(level='serial_number').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('selector', SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None,...state=None, verbose=True, warm_start=False),\n",
       "        norm_order=1, prefit=False, threshold='mean'))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(fm, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the selected features from the pipeline and subselect the feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = set(fm.loc[:, pipeline.steps[-1][1].get_support()].columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_selected = [f for f in fl if f.get_name() in selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save for reuse in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.save_features(fl_selected, \"fl_backblaze_selected.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the important features from the original feature matrix/tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftens = ftens[[f.get_name() for f in fl]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftens.to_csv('backblaze_ftens.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DLDB with desired hyperparameters\n",
    "\n",
    "In this example, we use 2 fairly small [LSTM](https://keras.io/layers/recurrent/) layers and 2 feed-forward layers (called \"Dense layers\" in Keras/Tensor Flow terminology). DLDB has an extremely simple API, and exposes a large number of hyperparameters, so is amenable to hyperparameter optimization algorithms.\n",
    "\n",
    "Each categorical feature will be mapped to a 12-dimensional embedding, with a maximum of 20 unique categorical values (the top 20 most frequent values will be chosen, and the rest will be converted to a single token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model = DLDB(\n",
    "    regression=False,\n",
    "    classes=[False, True],\n",
    "    recurrent_layer_sizes=(32, 32),\n",
    "    dense_layer_sizes=(32, 16),\n",
    "    dropout_fraction=0.2,\n",
    "    recurrent_dropout_fraction=0.2,\n",
    "    categorical_embedding_size=12,\n",
    "    categorical_max_vocab=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and test using cross-validation\n",
    "\n",
    "We use a `batch_size` of 128 (for each gradient update step) and train over 3 passes of the dataset (epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits=7\n",
    "splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7/8 [=========================>....] - ETA: 3s - loss: 0.6418\n",
      "8/8 [==============================] - 27s 3s/step - loss: 0.6206\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.5577\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.5289\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.6439424230307876\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.6576h 1/\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.5874\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.5606\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.8159736105557777\n",
      "Epoch 1/3\n",
      "7/8 [=========================>....] - ETA: 2s - loss: 0.6477\n",
      "8/8 [==============================] - 19s 2s/step - loss: 0.6291\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.5770\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.5444\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.7424586776859503\n",
      "Epoch 1/3\n",
      "7/8 [=========================>....] - ETA: 2s - loss: 0.6583Epoch 1/3\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.6423\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5759\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5587\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.6475206611570248\n",
      "Epoch 1/3\n",
      "7/8 [=========================>....] - ETA: 1s - loss: 0.6569Epoch 1/3\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6419\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.5691\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.5453\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.8011363636363636\n",
      "Epoch 1/3\n",
      "7/8 [=========================>....] - ETA: 1s - loss: 0.6912Epoch 1/3\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6811\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.6084\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5747\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.762293388429752\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.6445\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.5827\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.5564\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.6922520661157024\n",
      "DFS AUC 0.73 +/- 0.05\n"
     ]
    }
   ],
   "source": [
    "cv_score = []\n",
    "\n",
    "for train_test_index in splitter.split(labels, labels):\n",
    "    train_labels = labels.reset_index('cutoff', drop=True).iloc[train_test_index[0]]\n",
    "    test_labels = labels.reset_index('cutoff', drop=True).iloc[train_test_index[1]]\n",
    "    train_ftens = ftens.reset_index('time', drop=True).loc[train_labels.index, :]\n",
    "    test_ftens = ftens.reset_index('time', drop=True).loc[test_labels.index, :]\n",
    "\n",
    "    dl_model.fit(\n",
    "        train_ftens, train_labels, fl=fl,\n",
    "        batch_size=128,\n",
    "        # Set this to number of cores\n",
    "        workers=8,\n",
    "        use_multiprocessing=True,\n",
    "        shuffle=False,\n",
    "        epochs=3)\n",
    "\n",
    "    predictions = dl_model.predict(test_ftens)\n",
    "    score = roc_auc_score(test_labels, predictions)\n",
    "    print(\"cv score: \", score)\n",
    "    cv_score.append(score)\n",
    "mean_score = np.mean(cv_score)\n",
    "stderr = 2 * (np.std(cv_score) / np.sqrt(n_splits))\n",
    "\n",
    "print(\"DFS AUC %.2f +/- %.2f\" % (mean_score, stderr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
