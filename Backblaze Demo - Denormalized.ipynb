{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils_backblaze as utils\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from dldb import DLDB\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in the data\n",
    "\n",
    "The data is loaded from many individual CSV files, and then cut off at times specified by the labels.\n",
    "\n",
    "To make this notebook more interactive and because the data is heavily imbalanced toward working hard drives, we downsample the \"negative class\". A positive label means that a hard drive failed on the subsequent day, while a negative means that it did not. To do this downsampling, we remove 90% of the hard drives that never failed across the duration of the available CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/bschreck/Google Drive File Stream/My Drive/Feature Labs Shared/EntitySets/entitysets/backblaze_harddrive/data\"\n",
    "df = utils.load_data_as_dataframe(data_dir=data_dir, csv_glob='*.csv',\n",
    "                                  negative_downsample_frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bschreck/miniconda3/envs/py3default/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: 'serial_number' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    853\n",
       "True     321\n",
       "Name: failure, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('serial_number')['failure'].last().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to create an EntitySet in this notebook, because the labeling utility function depends on it. In general, you shouldn't need to create an EntitySet to just run DLDB on a denormalized dataset. But it might be helpful to simplify/unify your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: BackBlaze\n",
       "  Entities:\n",
       "    SMART_observations [Rows: 67424, Columns: 94]\n",
       "    HDD [Rows: 1174, Columns: 4]\n",
       "    models [Rows: 27, Columns: 1]\n",
       "  Relationships:\n",
       "    SMART_observations.serial_number -> HDD.serial_number\n",
       "    HDD.model -> models.model"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = utils.load_entityset_from_dataframe(df)\n",
    "es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_window = \"20 days\"\n",
    "lead = pd.Timedelta('1 day')\n",
    "prediction_window = pd.Timedelta('25 days')\n",
    "min_training_data = pd.Timedelta('5 days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating labels...: 100%|██████████| 1175/1175 [00:03<00:00, 360.44it/s]\n"
     ]
    }
   ],
   "source": [
    "labels = utils.create_labels(es,\n",
    "                             lead,\n",
    "                             min_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    852\n",
       "True     282\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_raw = utils.cutoff_raw_data(df, labels, training_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DLDB with desired hyperparameters\n",
    "\n",
    "In this example, we use 2 fairly small [LSTM](https://keras.io/layers/recurrent/) layers and 2 feed-forward layers (called \"Dense layers\" in Keras/Tensor Flow terminology). DLDB has an extremely simple API, and exposes a large number of hyperparameters, so is amenable to hyperparameter optimization algorithms.\n",
    "\n",
    "Each categorical feature will be mapped to a 12-dimensional embedding, with a maximum of 20 unique categorical values (the top 20 most frequent values will be chosen, and the rest will be converted to a single token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model = DLDB(\n",
    "    regression=False,\n",
    "    classes=[False, True],\n",
    "    recurrent_layer_sizes=(32, 32),\n",
    "    dense_layer_sizes=(32, 16),\n",
    "    dropout_fraction=0.2,\n",
    "    recurrent_dropout_fraction=0.2,\n",
    "    categorical_embedding_size=12,\n",
    "    categorical_max_vocab=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and test using cross-validation\n",
    "\n",
    "We use a `batch_size` of 128 (for each gradient update step) and train over 3 passes of the dataset (epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits=7\n",
    "splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we tell DL-DB explicitly what feature names are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "8/8 [==============================] - 5s 618ms/step - loss: 0.6507\n",
      "Epoch 2/3\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 1s 133ms/step - loss: 0.6028\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.5853\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.5981607357057177\n",
      "Epoch 1/3\n",
      "7/8 [=========================>....] - ETA: 2s - loss: 0.6863\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.6756\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 1s 134ms/step - loss: 0.6294\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 0.6050\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.5269892043182727\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6831\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 2s 188ms/step - loss: 0.6464\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.631 - 1s 100ms/step - loss: 0.6186\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.4364754098360656\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.6406\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 1s 163ms/step - loss: 0.5888\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 1s 169ms/step - loss: 0.5638\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.6380122950819671\n",
      "Epoch 1/3\n",
      "7/8 [=========================>....] - ETA: 1s - loss: 0.6713Epoch 1/3\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.6609\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.6175\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 1s 171ms/step - loss: 0.5911\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.6737704918032787\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 8s 973ms/step - loss: 0.6893\n",
      "Epoch 2/3\n",
      "\n",
      "8/8 [==============================] - 1s 128ms/step - loss: 0.6384\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 1s 112ms/step - loss: 0.5962\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.4925619834710744\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.6628h 1/\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 0.6046\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.5754\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.5836776859504132\n",
      "DENORM AUC 0.56 +/- 0.06\n"
     ]
    }
   ],
   "source": [
    "cv_score = []\n",
    "\n",
    "for i, train_test_index in enumerate(splitter.split(labels, labels)):\n",
    "    train_labels = labels.reset_index('cutoff', drop=True).iloc[train_test_index[0]]\n",
    "    test_labels = labels.reset_index('cutoff', drop=True).iloc[train_test_index[1]]\n",
    "    train_ftens = cutoff_raw.reset_index('date', drop=True).loc[train_labels.index, :]\n",
    "    test_ftens = cutoff_raw.reset_index('date', drop=True).loc[test_labels.index, :]\n",
    "\n",
    "    dl_model.fit(\n",
    "        train_ftens, train_labels,\n",
    "        categorical_feature_names=['model'],\n",
    "        batch_size=128,\n",
    "        # Set this to number of cores\n",
    "        workers=8,\n",
    "        use_multiprocessing=True,\n",
    "        shuffle=False,\n",
    "        epochs=3)\n",
    "\n",
    "    predictions = dl_model.predict(test_ftens)\n",
    "    score = roc_auc_score(test_labels, predictions)\n",
    "    print(\"cv score: \", score)\n",
    "    cv_score.append(score)\n",
    "\n",
    "mean_score = np.mean(cv_score)\n",
    "stderr = 2 * (np.std(cv_score) / np.sqrt(n_splits))\n",
    "\n",
    "print(\"DENORM AUC %.2f +/- %.2f\" % (mean_score, stderr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
