{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bschreck/miniconda3/envs/py3default/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.1.19'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import featuretools as ft\n",
    "from featuretools.primitives import NumTrue, PercentTrue\n",
    "from featuretools.selection import remove_low_information_features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils_backblaze as utils\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import (RandomForestClassifier,\n",
    "                              RandomForestRegressor)\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from dldb import DLDB\n",
    "import os\n",
    "ft.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLDB: Using DFS for Train Recurrent Neural Networks\n",
    "\n",
    "\n",
    "### Brief DFS primer\n",
    "Deep Feature Synthesis (DFS) works on time-varying, transactional-level data to generate powerful, interpretable features for machine learning. Raw data consists of many tables, with some columns acting as links between tables. We want to produce a fixed-size feature vector for each row of one of these tables, but taking advantage of the data contained in the other tables. DFS generates these feature vectors by applying many statistical functions, called primitives, across tables. And importantly, it generates these features at specific moments in time, taking precautions to only use data from before the desired time.\n",
    "\n",
    "For instance, the data we will use in this notebook contains a table with a row for each Instacart user, and several other tables about their shopping behavior. DFS can apply the \"sum\" primitive to the dollar amount of each order per user, producing a feature for \"the total amount spent on Instacart per user\". Adding a *cutoff time* of March 1, 2015, the feature becomes the \"total amount spent on Instacart per user before March 1, 2015\". DFS can also combine several primitives, allowing it to form features like the \"standard deviation of the number of items in each user's previous orders\".\n",
    "\n",
    "For a more in depth explanation of DFS, we encourage you to check out this [blog post](https://www.featurelabs.com/blog/deep-feature-synthesis/) and [this page](https://docs.featuretools.com/automated_feature_engineering/afe.html) in the Featuretools documentation.\n",
    "\n",
    "### Producing a 3D tensor from DFS\n",
    "DFS as described produces a 2-dimensional feature matrix that can be used for classic machine learning techniques, such as SVM or Random Forest. These techniques need a fixed-size feature matrix where any time-dependence is summarized into historical statistics (e.g. Number of items a customer purchased in the past 30 days).\n",
    "\n",
    "We can take more explicit advantage of the time-dimension in this type of data using Recurrent Neural Networks. RNNs take in sequences of features, where the 3rd dimension in our case would represent time. Since RNNs learn high-level features on their own, the usual approach when using multiple tables is just to join all of them together and use the raw values. We will show that approach as a baseline here.\n",
    "\n",
    "Instead, we can use DFS to produce high-level features at different points in time, and then learn from these sequences of features, rather than raw data. In this case, we would use DFS to produce a 3D tensor flattened as a 2D matrix, with multiple times for each instance. Combining DFS with RNNs essentially encodes prior human intuition and assumptions about relevant data transformations into the problem before letting the deep learning do its thing. Because the net doesn't have to learn every feature from scratch, we may be able to reduce training time, use a simpler net, not have to tweak as many hyperparameters, use less data, or boost performance. In this notebook, we will show a network using DFS features that produces higher scores with less variation than the same network trained on raw data.\n",
    "\n",
    "We'll try it on [harddrive failure data from Backblaze](https://www.backblaze.com/b2/hard-drive-test-data.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief DFS Primer\n",
    "\n",
    "The DFS algorithm is a way to build high-level features from raw time-varying \n",
    "\n",
    "For a more in depth explanation, we encourage you to check out this [blog post](https://www.featurelabs.com/blog/deep-feature-synthesis/) and [this page](https://docs.featuretools.com/automated_feature_engineering/afe.html) in the Featuretools documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLDB Library\n",
    "\n",
    "[DLDB](https://github.com/HDI-Project/DL-DB) is a utility library for building recurrent neural networks from a feature matrix with multiple cutoff times per instance. Internally, it uses the [Keras](keras.io) library (which in turn uses [Tensor Flow](tensorflow.org)). \n",
    "\n",
    "It works by first imputing and scaling a sequence feature matrix (the result of calling `tdfs()`), and then separating the numeric features from the categoricals. Each categorical feature is mapped to a Keras Embedding layer in order to transform it into a dense, numeric vector. Then these embeddings are concatenated with the numeric features and fed into several recurrent layers (specified in hyperparameters) and several feed-forward layers (also specified in hyperparameters). It also includes an optional 1-D convolutional layer that will be applied before the recurrent layers. All the network layers, including the categorical embeddings, are trained end-to-end using any gradient update methods available in Keras.\n",
    "\n",
    "We packaged DL-DB into a Python library that can be installed via pip:\n",
    "    \n",
    "```\n",
    "pip install dldb\n",
    "```\n",
    "\n",
    "This library includes both a class to build these recurrent neural network models as well as the `tdfs()` function that creates time-series features as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DLDB_updated_fig.png](attachment:DLDB_updated_fig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in the data\n",
    "\n",
    "The data is loaded from many individual CSV files, and then converted into the Featuretools Entityset format.\n",
    "\n",
    "To make this notebook more interactive and because the data is heavily imbalanced toward working hard drives, we downsample the \"negative class\". A positive label means that a hard drive failed on the subsequent day, while a negative means that it did not. To do this downsampling, we remove 90% of the hard drives that never failed across the duration of the available CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Volumes/Untitled/FeatureLabs/datasets/backblaze/data_Q1_2017'\n",
    "df = utils.load_data_as_dataframe(data_dir=data_dir, csv_glob='*.csv',\n",
    "                                  negative_downsample_frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bschreck/miniconda3/envs/py3default/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: 'serial_number' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    897\n",
       "True     384\n",
       "Name: failure, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('serial_number')['failure'].last().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: BackBlaze\n",
       "  Entities:\n",
       "    SMART_observations [Rows: 86241, Columns: 94]\n",
       "    HDD [Rows: 1281, Columns: 4]\n",
       "    models [Rows: 26, Columns: 1]\n",
       "  Relationships:\n",
       "    SMART_observations.serial_number -> HDD.serial_number\n",
       "    HDD.model -> models.model"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = utils.load_entityset_from_dataframe(df)\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct labels\n",
    "\n",
    "This utility function picks out a sampling of hard drives at particular days in their lifecycle, and labels each as True or False depending on whether they failed the following day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_window = \"20 days\"\n",
    "lead = pd.Timedelta('1 day')\n",
    "prediction_window = pd.Timedelta('25 days')\n",
    "min_training_data = pd.Timedelta('5 days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating labels...: 100%|██████████| 1282/1282 [00:03<00:00, 413.97it/s]\n"
     ]
    }
   ],
   "source": [
    "labels = utils.create_labels(es,\n",
    "                             lead,\n",
    "                             min_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    894\n",
       "True     342\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create time-stamped feature tensor using DFS\n",
    "\n",
    "Here is where things start to get interesting. We use the [`make_temporal_cutoffs` function in Featuretools](https://github.com/HDI-Project/DL-DB/blob/master/dldb/tdfs.py) to produce a serious of preceding cutoff times for each label/cutoff time pair. We then provide these cutoffs to DFS to generate a feature tensor with several rows per harddrive serial number.\n",
    "\n",
    "This `make_temporal_cutoffs` function has a few different ways of selecting these additional cutoff times. Here, we provide `window_size='1d'` and `starts` equal to the first recorded time for each drive. This produces sequences spaced out by 1 day (the frequency of recording in the actual dataset) from the first recording until the cutoff time, at which point we have to make a prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_ids = labels.index.get_level_values('serial_number')\n",
    "cutoffs = labels.index.get_level_values('cutoff')\n",
    "starts = es['SMART_observations'].df.groupby('serial_number')['date'].min().loc[instance_ids].tolist()\n",
    "temporal_cutoffs = ft.make_temporal_cutoffs(instance_ids=instance_ids,\n",
    "                                            cutoffs=cutoffs,\n",
    "                                            start=starts,\n",
    "                                            window_size='1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 1117 features\n",
      "Elapsed: 49:03 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 11/11 chunks\n"
     ]
    }
   ],
   "source": [
    "trans_primitives = [\"day\", \"days\"]\n",
    "label_feature = ft.Feature(es[\"SMART_observations\"][\"failure\"]) == 1\n",
    "seed_features = [label_feature,\n",
    "                 NumTrue(label_feature, es[\"HDD\"]), \n",
    "                 PercentTrue(label_feature, es[\"HDD\"])]\n",
    "ftens, fl = ft.dfs(entityset=es,\n",
    "                target_entity=\"HDD\",\n",
    "                cutoff_time=temporal_cutoffs,\n",
    "                cutoff_time_in_index=True,\n",
    "                trans_primitives=[\"day\", \"days\"],\n",
    "                seed_features=seed_features,\n",
    "                max_depth=2,\n",
    "                verbose=True)\n",
    "# Make sure ftens is sorted the same way as the labels\n",
    "ftens = ftens.swaplevel(i=1, j=0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>capacity_bytes</th>\n",
       "      <th>SUM(SMART_observations.smart_10_normalized)</th>\n",
       "      <th>SUM(SMART_observations.smart_10_raw)</th>\n",
       "      <th>SUM(SMART_observations.smart_11_normalized)</th>\n",
       "      <th>SUM(SMART_observations.smart_11_raw)</th>\n",
       "      <th>SUM(SMART_observations.smart_12_normalized)</th>\n",
       "      <th>SUM(SMART_observations.smart_12_raw)</th>\n",
       "      <th>SUM(SMART_observations.smart_13_normalized)</th>\n",
       "      <th>SUM(SMART_observations.smart_13_raw)</th>\n",
       "      <th>...</th>\n",
       "      <th>models.STD(HDD.NUM_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.STD(HDD.PERCENT_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.MAX(HDD.NUM_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.MAX(HDD.PERCENT_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.SKEW(HDD.NUM_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.SKEW(HDD.PERCENT_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.MIN(HDD.NUM_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.MIN(HDD.PERCENT_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.MEAN(HDD.NUM_TRUE(SMART_observations.failure = 1))</th>\n",
       "      <th>models.MEAN(HDD.PERCENT_TRUE(SMART_observations.failure = 1))</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th>serial_number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2017-01-01</th>\n",
       "      <th>4591K0GOFMYB</th>\n",
       "      <td>TOSHIBA MD04ABA400V</td>\n",
       "      <td>4.000787e+12</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45C8K0QLFMYB</th>\n",
       "      <td>TOSHIBA MD04ABA400V</td>\n",
       "      <td>4.000787e+12</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45CHK11WFMYB</th>\n",
       "      <td>TOSHIBA MD04ABA400V</td>\n",
       "      <td>4.000787e+12</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66GHS5NHS</th>\n",
       "      <td>TOSHIBA MQ01ABF050</td>\n",
       "      <td>5.001079e+11</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6VYA6RMB</th>\n",
       "      <td>ST3160318AS</td>\n",
       "      <td>1.600419e+11</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        model  capacity_bytes  \\\n",
       "time       serial_number                                        \n",
       "2017-01-01 4591K0GOFMYB   TOSHIBA MD04ABA400V    4.000787e+12   \n",
       "           45C8K0QLFMYB   TOSHIBA MD04ABA400V    4.000787e+12   \n",
       "           45CHK11WFMYB   TOSHIBA MD04ABA400V    4.000787e+12   \n",
       "           66GHS5NHS       TOSHIBA MQ01ABF050    5.001079e+11   \n",
       "           6VYA6RMB               ST3160318AS    1.600419e+11   \n",
       "\n",
       "                          SUM(SMART_observations.smart_10_normalized)  \\\n",
       "time       serial_number                                                \n",
       "2017-01-01 4591K0GOFMYB                                         100.0   \n",
       "           45C8K0QLFMYB                                         100.0   \n",
       "           45CHK11WFMYB                                         100.0   \n",
       "           66GHS5NHS                                            100.0   \n",
       "           6VYA6RMB                                             100.0   \n",
       "\n",
       "                          SUM(SMART_observations.smart_10_raw)  \\\n",
       "time       serial_number                                         \n",
       "2017-01-01 4591K0GOFMYB                                    0.0   \n",
       "           45C8K0QLFMYB                                    0.0   \n",
       "           45CHK11WFMYB                                    0.0   \n",
       "           66GHS5NHS                                       0.0   \n",
       "           6VYA6RMB                                        0.0   \n",
       "\n",
       "                          SUM(SMART_observations.smart_11_normalized)  \\\n",
       "time       serial_number                                                \n",
       "2017-01-01 4591K0GOFMYB                                           0.0   \n",
       "           45C8K0QLFMYB                                           0.0   \n",
       "           45CHK11WFMYB                                           0.0   \n",
       "           66GHS5NHS                                              0.0   \n",
       "           6VYA6RMB                                               0.0   \n",
       "\n",
       "                          SUM(SMART_observations.smart_11_raw)  \\\n",
       "time       serial_number                                         \n",
       "2017-01-01 4591K0GOFMYB                                    0.0   \n",
       "           45C8K0QLFMYB                                    0.0   \n",
       "           45CHK11WFMYB                                    0.0   \n",
       "           66GHS5NHS                                       0.0   \n",
       "           6VYA6RMB                                        0.0   \n",
       "\n",
       "                          SUM(SMART_observations.smart_12_normalized)  \\\n",
       "time       serial_number                                                \n",
       "2017-01-01 4591K0GOFMYB                                         100.0   \n",
       "           45C8K0QLFMYB                                         100.0   \n",
       "           45CHK11WFMYB                                         100.0   \n",
       "           66GHS5NHS                                            100.0   \n",
       "           6VYA6RMB                                             100.0   \n",
       "\n",
       "                          SUM(SMART_observations.smart_12_raw)  \\\n",
       "time       serial_number                                         \n",
       "2017-01-01 4591K0GOFMYB                                    1.0   \n",
       "           45C8K0QLFMYB                                   10.0   \n",
       "           45CHK11WFMYB                                    2.0   \n",
       "           66GHS5NHS                                       3.0   \n",
       "           6VYA6RMB                                       26.0   \n",
       "\n",
       "                          SUM(SMART_observations.smart_13_normalized)  \\\n",
       "time       serial_number                                                \n",
       "2017-01-01 4591K0GOFMYB                                           0.0   \n",
       "           45C8K0QLFMYB                                           0.0   \n",
       "           45CHK11WFMYB                                           0.0   \n",
       "           66GHS5NHS                                              0.0   \n",
       "           6VYA6RMB                                               0.0   \n",
       "\n",
       "                          SUM(SMART_observations.smart_13_raw)  \\\n",
       "time       serial_number                                         \n",
       "2017-01-01 4591K0GOFMYB                                    0.0   \n",
       "           45C8K0QLFMYB                                    0.0   \n",
       "           45CHK11WFMYB                                    0.0   \n",
       "           66GHS5NHS                                       0.0   \n",
       "           6VYA6RMB                                        0.0   \n",
       "\n",
       "                                                      ...                                \\\n",
       "time       serial_number                              ...                                 \n",
       "2017-01-01 4591K0GOFMYB                               ...                                 \n",
       "           45C8K0QLFMYB                               ...                                 \n",
       "           45CHK11WFMYB                               ...                                 \n",
       "           66GHS5NHS                                  ...                                 \n",
       "           6VYA6RMB                                   ...                                 \n",
       "\n",
       "                          models.STD(HDD.NUM_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                             \n",
       "2017-01-01 4591K0GOFMYB                                                 0.0          \n",
       "           45C8K0QLFMYB                                                 0.0          \n",
       "           45CHK11WFMYB                                                 0.0          \n",
       "           66GHS5NHS                                                    0.0          \n",
       "           6VYA6RMB                                                     0.0          \n",
       "\n",
       "                          models.STD(HDD.PERCENT_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                                 \n",
       "2017-01-01 4591K0GOFMYB                                                 0.0              \n",
       "           45C8K0QLFMYB                                                 0.0              \n",
       "           45CHK11WFMYB                                                 0.0              \n",
       "           66GHS5NHS                                                    0.0              \n",
       "           6VYA6RMB                                                     0.0              \n",
       "\n",
       "                          models.MAX(HDD.NUM_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                             \n",
       "2017-01-01 4591K0GOFMYB                                                 0.0          \n",
       "           45C8K0QLFMYB                                                 0.0          \n",
       "           45CHK11WFMYB                                                 0.0          \n",
       "           66GHS5NHS                                                    0.0          \n",
       "           6VYA6RMB                                                     0.0          \n",
       "\n",
       "                          models.MAX(HDD.PERCENT_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                                 \n",
       "2017-01-01 4591K0GOFMYB                                                 0.0              \n",
       "           45C8K0QLFMYB                                                 0.0              \n",
       "           45CHK11WFMYB                                                 0.0              \n",
       "           66GHS5NHS                                                    0.0              \n",
       "           6VYA6RMB                                                     0.0              \n",
       "\n",
       "                          models.SKEW(HDD.NUM_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                              \n",
       "2017-01-01 4591K0GOFMYB                                                 0.0           \n",
       "           45C8K0QLFMYB                                                 0.0           \n",
       "           45CHK11WFMYB                                                 0.0           \n",
       "           66GHS5NHS                                                    0.0           \n",
       "           6VYA6RMB                                                     0.0           \n",
       "\n",
       "                          models.SKEW(HDD.PERCENT_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                                  \n",
       "2017-01-01 4591K0GOFMYB                                                 0.0               \n",
       "           45C8K0QLFMYB                                                 0.0               \n",
       "           45CHK11WFMYB                                                 0.0               \n",
       "           66GHS5NHS                                                    0.0               \n",
       "           6VYA6RMB                                                     0.0               \n",
       "\n",
       "                          models.MIN(HDD.NUM_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                             \n",
       "2017-01-01 4591K0GOFMYB                                                 0.0          \n",
       "           45C8K0QLFMYB                                                 0.0          \n",
       "           45CHK11WFMYB                                                 0.0          \n",
       "           66GHS5NHS                                                    0.0          \n",
       "           6VYA6RMB                                                     0.0          \n",
       "\n",
       "                          models.MIN(HDD.PERCENT_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                                 \n",
       "2017-01-01 4591K0GOFMYB                                                 0.0              \n",
       "           45C8K0QLFMYB                                                 0.0              \n",
       "           45CHK11WFMYB                                                 0.0              \n",
       "           66GHS5NHS                                                    0.0              \n",
       "           6VYA6RMB                                                     0.0              \n",
       "\n",
       "                          models.MEAN(HDD.NUM_TRUE(SMART_observations.failure = 1))  \\\n",
       "time       serial_number                                                              \n",
       "2017-01-01 4591K0GOFMYB                                                 0.0           \n",
       "           45C8K0QLFMYB                                                 0.0           \n",
       "           45CHK11WFMYB                                                 0.0           \n",
       "           66GHS5NHS                                                    0.0           \n",
       "           6VYA6RMB                                                     0.0           \n",
       "\n",
       "                          models.MEAN(HDD.PERCENT_TRUE(SMART_observations.failure = 1))  \n",
       "time       serial_number                                                                 \n",
       "2017-01-01 4591K0GOFMYB                                                 0.0              \n",
       "           45C8K0QLFMYB                                                 0.0              \n",
       "           45CHK11WFMYB                                                 0.0              \n",
       "           66GHS5NHS                                                    0.0              \n",
       "           6VYA6RMB                                                     0.0              \n",
       "\n",
       "[5 rows x 1117 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting features\n",
    "DFS generates over 1000 features for this dataset. Many of them won't be useful, so we an do a pass of supervised feature selection before building the deep learning model.\n",
    "\n",
    "To do this, we use a Random Forest Classifier's built-in feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, one-hot-encode categoricals and drop zero-variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ftens, fl = ft.encode_features(ftens, fl)\n",
    "ftens, fl = remove_low_information_features(ftens, fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, impute missing values and train a Random Forest on the last cutoff time for each hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "est = RandomForestClassifier(n_estimators=1000, class_weight='balanced', n_jobs=-1, verbose=True)\n",
    "imputer = Imputer(missing_values='NaN', strategy=\"mean\", axis=0)\n",
    "selector = SelectFromModel(est, threshold=\"mean\")\n",
    "pipeline = Pipeline([(\"imputer\", imputer),(\"selector\", selector)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fm = ftens.groupby(level='serial_number').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('selector', SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None,...state=None, verbose=True, warm_start=False),\n",
       "        norm_order=1, prefit=False, threshold='mean'))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(fm, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the selected features from the pipeline and subselect the feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected = set(fm.loc[:, pipeline.steps[-1][1].get_support()].columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fl_selected = [f for f in fl if f.get_name() in selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save for reuse in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ft.save_features(fl_selected, \"fl_backblaze_selected.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the important features from the original feature matrix/tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ftens = ftens[[f.get_name() for f in fl]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Baseline Input Data\n",
    "\n",
    "We cutoff the raw data at the same time points as we used for DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutoff_raw = utils.cutoff_raw_data(df, labels, training_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DLDB with desired hyperparameters\n",
    "\n",
    "In this example, we use 2 fairly small [LSTM](https://keras.io/layers/recurrent/) layers and 2 feed-forward layers (called \"Dense layers\" in Keras/Tensor Flow terminology). DLDB has an extremely simple API, and exposes a large number of hyperparameters, so is amenable to hyperparameter optimization algorithms.\n",
    "\n",
    "Each categorical feature will be mapped to a 12-dimensional embedding, with a maximum of 20 unique categorical values (the top 20 most frequent values will be chosen, and the rest will be converted to a single token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dl_model = DLDB(\n",
    "    regression=False,\n",
    "    classes=[False, True],\n",
    "    recurrent_layer_sizes=(32, 32),\n",
    "    dense_layer_sizes=(32, 16),\n",
    "    dropout_fraction=0.2,\n",
    "    recurrent_dropout_fraction=0.2,\n",
    "    categorical_embedding_size=12,\n",
    "    categorical_max_vocab=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and test using cross-validation\n",
    "\n",
    "We use a `batch_size` of 128 (for each gradient update step) and train over 3 passes of the dataset (epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_splits=7\n",
    "splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "9/9 [==============================] - 20s 2s/step - loss: 0.6645\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 15s 2s/step - loss: 0.6048\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 14s 2s/step - loss: 0.5646\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.6829559948979592\n",
      "Epoch 1/3\n",
      "8/9 [=========================>....] - ETA: 2s - loss: 0.6448Epoch 1/3\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.6310\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 15s 2s/step - loss: 0.5693\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 14s 2s/step - loss: 0.5605\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.7942442602040816\n",
      "Epoch 1/3\n",
      "8/9 [=========================>....] - ETA: 2s - loss: 0.66747\n",
      "9/9 [==============================] - 17s 2s/step - loss: 0.6582\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 13s 1s/step - loss: 0.5934\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 13s 1s/step - loss: 0.5586\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.6979432397959183\n",
      "Epoch 1/3\n",
      "8/9 [=========================>....] - ETA: 1s - loss: 0.6644Epoch 1/3\n",
      "9/9 [==============================] - 16s 2s/step - loss: 0.6564\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 13s 1s/step - loss: 0.5796\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 13s 1s/step - loss: 0.5424\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.6238839285714286\n",
      "Epoch 1/3\n",
      "8/9 [=========================>....] - ETA: 1s - loss: 0.6700Epoch 1/3\n",
      "9/9 [==============================] - 16s 2s/step - loss: 0.6588\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 13s 1s/step - loss: 0.5968\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 14s 2s/step - loss: 0.5775\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.6893335459183673\n",
      "Epoch 1/3\n",
      "8/9 [=========================>....] - ETA: 1s - loss: 0.6914Epoch 1/3\n",
      "9/9 [==============================] - 16s 2s/step - loss: 0.6869\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 13s 1s/step - loss: 0.6272\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 13s 1s/step - loss: 0.6102\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.7692431303229953\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.6494\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 14s 2s/step - loss: 0.5729\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 14s 2s/step - loss: 0.5521\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.7476213910761154\n",
      "DFS AUC 0.72 +/- 0.04\n"
     ]
    }
   ],
   "source": [
    "cv_score = []\n",
    "\n",
    "for train_test_index in splitter.split(labels, labels):\n",
    "    train_labels = labels.reset_index('cutoff', drop=True).iloc[train_test_index[0]]\n",
    "    test_labels = labels.reset_index('cutoff', drop=True).iloc[train_test_index[1]]\n",
    "    train_ftens = ftens.reset_index('time', drop=True).loc[train_labels.index, :]\n",
    "    test_ftens = ftens.reset_index('time', drop=True).loc[test_labels.index, :]\n",
    "\n",
    "    dl_model.fit(\n",
    "        train_ftens, train_labels, fl=fl,\n",
    "        batch_size=128,\n",
    "        workers=8,\n",
    "        use_multiprocessing=True,\n",
    "        shuffle=False,\n",
    "        epochs=3)\n",
    "\n",
    "    predictions = dl_model.predict(test_ftens)\n",
    "    score = roc_auc_score(test_labels, predictions)\n",
    "    print(\"cv score: \", score)\n",
    "    cv_score.append(score)\n",
    "mean_score = np.mean(cv_score)\n",
    "stderr = 2 * (np.std(cv_score) / np.sqrt(n_splits))\n",
    "\n",
    "print(\"DFS AUC %.2f +/- %.2f\" % (mean_score, stderr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the baseline model over raw data and test using cross-validation\n",
    "\n",
    "We use the same parameters here. Note that we tell DL-DB explicitly what feature names are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6863Epoch 1/3\n",
      "9/9 [==============================] - 5s 594ms/step - loss: 0.6814\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.6365\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 1s 148ms/step - loss: 0.6141\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.5668845663265306\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 5s 576ms/step - loss: 0.6735\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 1s 95ms/step - loss: 0.6221\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 1s 115ms/step - loss: 0.6004\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.6448501275510203\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 7s 731ms/step - loss: 0.6485\n",
      "Epoch 2/3\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 1s 90ms/step - loss: 0.6162\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 0.5949\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.6448501275510204\n",
      "Epoch 1/3\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6629Epoch 1/3\n",
      "9/9 [==============================] - 6s 650ms/step - loss: 0.6515\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 1s 76ms/step - loss: 0.5960\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 1s 108ms/step - loss: 0.5792\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.5927136479591837\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 6s 695ms/step - loss: 0.6920\n",
      "Epoch 2/3\n",
      "\n",
      "9/9 [==============================] - 1s 90ms/step - loss: 0.6676\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 0.6397\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.46269132653061223\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 6s 699ms/step - loss: 0.6497\n",
      "Epoch 2/3\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.6142\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 1s 88ms/step - loss: 0.5989\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.653061224489796\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 7s 784ms/step - loss: 0.6472\n",
      "Epoch 2/3\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 1s 110ms/step - loss: 0.6059\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 0.5878\n",
      "Transforming input tensor into numeric sequences\n",
      "Predicting using Keras model\n",
      "Transforming outputs\n",
      "cv score:  0.6587106299212598\n",
      "DENORM AUC 0.60 +/- 0.05\n"
     ]
    }
   ],
   "source": [
    "cv_score = []\n",
    "\n",
    "for i, train_test_index in enumerate(splitter.split(labels, labels)):\n",
    "    train_labels = labels.reset_index('cutoff', drop=True).iloc[train_test_index[0]]\n",
    "    test_labels = labels.reset_index('cutoff', drop=True).iloc[train_test_index[1]]\n",
    "    train_ftens = cutoff_raw.reset_index('date', drop=True).loc[train_labels.index, :]\n",
    "    test_ftens = cutoff_raw.reset_index('date', drop=True).loc[test_labels.index, :]\n",
    "\n",
    "    dl_model.fit(\n",
    "        train_ftens, train_labels,\n",
    "        categorical_feature_names=['model'],\n",
    "        batch_size=128,\n",
    "        workers=8,\n",
    "        use_multiprocessing=True,\n",
    "        shuffle=False,\n",
    "        epochs=3)\n",
    "\n",
    "    predictions = dl_model.predict(test_ftens)\n",
    "    score = roc_auc_score(test_labels, predictions)\n",
    "    print(\"cv score: \", score)\n",
    "    cv_score.append(score)\n",
    "\n",
    "mean_score = np.mean(cv_score)\n",
    "stderr = 2 * (np.std(cv_score) / np.sqrt(n_splits))\n",
    "\n",
    "print(\"DENORM AUC %.2f +/- %.2f\" % (mean_score, stderr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling to the full dataset\n",
    "\n",
    "We used 3 months of data here, but really only. Typically, deep learning methods start to shine as the data gets bigger and bigger.\n",
    "\n",
    "The full dataset spans several years, and it would be interesting to see how the DFS-features compare to the raw data as more data is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "The model using DFS features scored about 20% better AUC than the raw data model for the same parameters, with slightly lower variation.\n",
    "\n",
    "This is an interesting result, and hints at the idea that using good features to start out with can possibly increase the scores of deep learning models or maybe reduce the training time. However, I did not attempt to tweak many of the parameters in the network (number of layers, size of layers, etc) or in training the network (e.g. number of epochs).\n",
    "\n",
    "There are many more ideas we can test here:\n",
    " * What happens as we increase the complexity of the network?\n",
    " * What if we introduce hyperparameter optimization?\n",
    " * Is it possible to visualize the effect of the input features on the LSTM network? This is a hard problem in deep learning in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
